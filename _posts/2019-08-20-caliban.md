---
layout: post
title:  "Caliban: Deploying Deep Learning Tools for Live-Cell Imaging"
excerpt: "Overview, motivation, and in-depth explanation of summer internship project 2019."
categories: [internship]
---



Deciphering the mechanisms of cell motility and morphology is essential to understanding a
variety of biological processes, including tumorigenesis, signaling dynamics, and cell fates in
embryonic development. The first critical step to analyzing these systems is to accurately
identify segments of cells and track their morphology over time. With the newest advances in
microscopy techniques, biologists are now able to distill information about cell structures and
behavior with increasing complexity [1]. The quantity of data generated by these tools is not
trivial; we are now able to create voluminous datasets of cellular images. However, all of these
images must be processed and labeled before they can become critical data points to extract
novel insights. This is a very intensive, time-consuming process and potentially demands lots of
intervention from biologists before they can use the data for analysis.


Recent improvements in computing techniques like deep learning allow researchers to automate
tasks like segmentation and tracking using intricate algorithms. These technologies have
great promise in accelerating and transforming the exploitation of biological data from live-cell
imaging [5]. However, deep learning is heavily mathematical and require complex computational
resources. As a result, it is a method that still poses significant difficulties for life and medical
scientists.


The Van Valen Lab is at the forefront of innovating tools to enable widespread adoption of deep
learning in biology. The lab leverages the latest advances in imaging and machine learning to
build scalable deep learning models and software analysis tools. These tools are hosted on
DeepCell 2.0, a cloud computing platform that can deploy deep learning models on large datasets
[6]. One of the tools that they are pioneering is a novel method that automatically segments and
tracks cells across in-vitro screening frames. This versatile method uses information from cells
tracked in previous frames – appearance, neighborhood, morphology, motion, etc. – to predict
the labels to track in the next frame. To create complex architectures like this, the lab needs large
quantities of quality annotated data. However, creating these datasets is a time-consuming
process and should not be a task that developers have to concurrently tackle.


Currently, leading platforms like Amazon Mechanical Turk, Figure Eight, and Amazon
SageMaker GroundTruth are the leading solutions to getting data annotated on a large scale. Key
to these services is crowdsourcing from tens of thousands of workers and independent
contractors [7]. These platforms allow developers to upload their raw data files, select the type of
annotation job they desire (i.e. object segmentation, audio transcription, bounding box), and pay
a small monetary reward to workers for each annotated data.


Typically, to train object tracking focused deep learning models, developers require annotated
datasets that give pixel-level segmentation and unique labels for each object and frame. Since
these tracking models are now commonly built, these data annotation platforms hold existing
templates to create these types of datasets. However, the problem that the lab is trying to solve is
much more complicated. Due to biological processes like cell mitosis and deaths, each unique
cell label also needs to record lineage information. Thus, better annotation tools are needed to
create quality training datasets for cell tracking and segmentation models. These tools currently
do not exist on any of the platforms.


To meet this challenge, I worked with the lab to launch a desktop software program called Caliban to
segment and track across cell image frames for human-in-the-loop data curation. My project this
summer was to design algorithmic functions for the desktop tool, containerize the software using Docker,
spearhead the construction of the web application for Caliban, and establish a data pipeline for
crowdsourcing platform integration. The software framework has been created to be flexible to
work on both time-lapse and 3D spatial frames. The integration of Caliban with Figure Eight will help life scientists move past
arduous methods of in-lab annotation techniques and focus on making imaging-based discoveries
instead with large ground-truth datasets. These tools will soon be deployed to the scientific
community to accelerate the construction of quality annotated data for deep learning modeling
and statistical analysis.


## Software Architecture
Caliban was created with an event-driven architecture in mind. The software tool itself makes
changes to the data file given after the user generates an “event” with mouse clicks or keyboard
operations. The “event” is then handed off to the code assigned to respond to that type of user
input. 

### Web-based Annotation Tool. 
The browser version of Caliban was created to allow for ease of
scalability as we bring in more external workers to help annotate data. This application was
created using the Python Flask framework, a popular web application framework used to build
websites and pages. The tool has been integrated with AWS S3 for data storage and organization
(explained more thoroughly in Object Storage section) . The application is currently being served
on AWS Elastic Beanstalk. Deployment to AWS EB automatically creates an EC2 instance, load
balancer, auto-scaling group, and other resources for dynamic sites [8].
Containerization. Containers allow a developer to package up an application with all of the parts
it needs, such as libraries and other dependencies, and ship it all out as one package [9]. This
way, the software can be run on Windows, Mac, and Unix-based operating systems without
needing to download necessary dependencies and library packages. Any user can simply build
the container, and run the image without worrying about manually setting up the proper
environment for the application. On command, the desktop tool can be quickly packaged to run
within a Docker container.


### User Interface. 
Caliban was designed so that users can quickly access lineage and spatial
information about each cell. To help with spatial annotations, users can toggle keys to access
both the raw image and the segmented image of each frame. An “edit mode” has been created to
make detailed corrections to the segmented masks of cells if necessary. Lineage information is
given when users hover their mouse over cell masks. Edits like mask deletion and swapping can
be initiated with predefined keyboard operations after selecting the desired cells.
Object Storage. For the desktop tool, the users can direct the application to look for requested
data in the local directories of their host system. For the browser version, we used AWS S3, an
object storage service that offers scalability, data availability, security, and performance. The
application takes advantage of boto3, the AWS Software Development Kit (SDK) for Python to
manage input and output S3 buckets.


## Methodology
This section will primarily focus on the construction of the browser-version application, the
template job of the Figure Eight task, along with details about containerizing the desktop
application.

### Training Data Files
Caliban was built to track cells across time-lapse and z-stack frames. As a
result, it was designed to accept two different data files for edit operations. Depending on the
input data file, the application will initiate the proper class instance to process the necessary
requests made by the user. As changes to the file are made through the user interface, the
application updates a nested dictionary of lineage and annotated mask information given by the
data files. To manually track and segment cells across frames, users need to be able to check the
accuracy of cells masks and lineage information. With this in mind, files meant to track cells
across time-lapse frames bundle zipped raw and segmented mask images (as NumPy 3D arrays
in the shape (frames, y, x)) and lineage information (as JSON) into a .trk file. Files meant to
track cells across z-stack frames bundle zipped raw and segmented mask images (as NumPy 4D
arrays in the shape (frames, y, x, channels or features)) into a .npz file.
Creating and Deploying the Web Application. Python Flask was used as a web application
framework for constructing Caliban. Flask allows developers to get applications running on the
browser and test code easily on a localhost port 5000 in a web browser when running with the
initiation command and the Flask library installed. The final Flask application has been deployed
to an AWS Elastic Beanstalk environment as a RESTful web service [8]. To do this, the Elastic
Beanstalk Command Line Interface (EB CLI) was configured on the local computer to set up and
update the deployed application when necessary. Using specific commands on the terminal,
AWS EB constructs the environment by creating an EC2 instance, load balancer, auto-scaling
group, among other resources. Instructions located at
https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create-deploy-python-flask.html
were followed very closely to deploy Flask applications to Elastic Beanstalk.

### Responding to User Operations
Caliban allows users to make key operations to edit the
metadata of lineage information. The Flask framework helps serves as the router that maps the
specific URL with the associated function that is intended to perform some task. Functions
depend on Python libraries-- including NumPy, Matplotlib, and scikit-learn – to change the
metadata for the files [10-14]. After the desired change has been made to the lineage information
or mask annotation, the Flask app routing will update the interface to reflect the alterations with
support from side-serving JavaScript scripts.


### Creating a Custom Job on Figure Eight 
Since no existing platforms can create the training data
we needed, our solution to this is to build a data annotation tool ourselves and direct external
workers to our tool. To do this, services like Amazon Mechanical Turk, Amazon SageMaker,
and Figure Eight all offer the ability for customers to create custom job templates. By hosting
Caliban on a web server, we can simply generate an iFrame on the job page for easy internal
viewing and have workers submit the name of the data file to verify that they completed the
correct job. An iFrame is an element that embeds an HTTPS page into the current one. The
embedded page needs to be secured with an encrypted website connection for it to be properly
viewed in the iFrame. To achieve that, a free certificate was obtained from Let’s Encrypt and
installed with help the Certbot ACME client. The instructions located at https://certbot.eff.org
were clear in how to properly set up the install the clients and issue the certificates.
Out of these three services, we chose to use Figure Eight to launch our jobs. At this time,
Amazon Mechanical Turk and Amazon SageMaker have key issues that prevent successful
implementation of crowdsourcing integration. For Amazon Mechanical Turk, the main issue is
that MTurkdoesn’t support IAM user account access on the user interface. Thus, if several
members of the lab want to submit batches of HITs through MTurk, this would mean they would
all have to be given access to the lab’s root user account, which is what the IAM credential
system is supposed to solve. Meanwhile, SageMaker currently does not have any worker quality
control methods, making it difficult to access a group of experienced workers who can generate
accurate annotations. Figure Eight allows us to do all of this. Within Figure Eight, one can use
the user interface to write the instructions and create scripts to request population of the iFrame.
A full description of the data annotation pipeline is shown with the figure below.


<figure>
  <img src="{{site.url}}/img/caliban_post/caliban_flow.png" alt="drawing"  width="400"/>
  <figcaption><center>Human-In-The-Loop Pipeline with Figure Eight. The production of ground truth data
using deep learning models is accelerated by having humans in the feedback loop of the
computational process. Using the raw live-imaging data, the tracking model will use previously
learned information to predict segmented masks and lineage information. The output from the
model can then be funneled to Figure Eight for further curation by external contributors or be
fixed by in-lab developers on the desktop tool. The final product is ground truth data, which can
then be used to send back to the model for retraining or be used for further biological analysis. (Figure constructed by the Van Valen Lab)</center></figcaption>
</figure>





### Containerization Using Docker
To set up the desktop version of Caliban to run in a Docker 
container, a Dockerfile was written to package software dependencies on command. To be able
to view the graphical application, we needed an X server, a window manager, and a means to
access the container's display. A bootstrap script was written to start Xvfb in the background, add
a Fluxbox window manager, and access the display using a VNC server. The Dockerfile uses
ubuntu:16.04 as the base image and installs necessary packages before calling the bootstrap
script. The README.md on the software repository page has been updated to include
instructions for running Docker in a container.


## Source Code and Detailed Software Instructions
A persistent deployment of the browser-based
annotation tool can be accessed at https://caliban.deepcell.org . All source code for both the
desktop and web application is available at https://github.com/vanvalenlab/caliban [15]. Detailed
instructions for how to use Caliban is located on the browser page and in the README section
of the GitHub repository. There is also a short 5-minute video on YouTube that shows some of
the operations in action. This can be viewed at
https://www.youtube.com/watch?v=EmEW3U2ta9Y.




## References 
1. Weigert, M. et al. Content-Aware Image Restoration: Pushing the Limits of Fluorescence
Microscopy. (2018).
2. LeCun, Y. et al . Deep learning. Nature 512, 436–444 (2015).
3. Jouppi, N. P. et al. In-datacenter performance analysis of a tensor processing unit. in
2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture
(ISCA) 1-12 (2017). doi:10.1145/3079856.3080246
4. Owens, J. D. et al. GPU Computing. Proc. IEEE 96, 879â€“899 (2008).
5. Van Valen, D. A. et al. Deep Learning Automates the Quantitative Analysis of Individual
Cells in Live-Cell Imaging Experiments. PLOS Comput. Biol. 12, e1005177 (2016).
6. Bannon, D. et al. DeepCell 2.0: Automated cloud deployment of deep learning models
for large-scale cellular image analysis. Preprint available at
biorxiv.org/content/10.1101/505032v3 .
7. Hughes, A. J. et al. Quanti.us: a tool for rapid, flexible, crowd-based annotation of
images. Nat. Methods 15, 587-590 (2018).
8. I. Bermudez, S. Traverso, M. Mellia and M. Munafò, "Exploring the cloud from passive
measurements: The Amazon AWS case," 2013 Proceedings IEEE INFOCOM , Turin,
2013, pp. 230-234. doi: 10.1109/INFCOM.2013.6566769
9. Merkel, D. Docker: Lightweight Linux Containers for Consistent Development and
Deployment. Linux J 2014, (2014).
10. S. van der Walt, S. C. Colbert and G. Varoquaux, "The NumPy Array: A Structure for
Efficient Numerical Computation," in Computing in Science & Engineering , vol. 13, no.
2, pp. 22-30, March-April 2011. doi: 10.1109/MCSE.2011.37
11. Hunter, J. D. Matplotlib: a 2D graphics environment. Comput. Sci. Eng. 9, 90–95 (2007).
12. Pedregosa, F. et al. Scikit-learn: machine learning in Python. J. Mach. Learn. Res. 12,
2825–2830 (2011).
13. Walt, S. van der et al. scikit-image: image processing in Python. PeerJ 2, e453 (2014).
14. Jones, E. et al. SciPy: open source scientific tools for Python. https://www. scipy.org/
(2001).
15. vanvalenlab/caliban. (vanvalenlab, 2019).
